# -*- coding: utf-8 -*-
"""logistics_regression_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A6LwMBUnARFgrBtOcyVMhXsAft32Fp_1
"""

pip install wget

import pandas as pd
import numpy as np
import wget

!wget "https://raw.githubusercontent.com/DeepConnectAI/challenge-week-3/master/data/diabetes_data.csv"

import random
import sympy as sym

A  = np.random.rand(10,10)
# inverse
inverse = np.linalg.inv(A)

# it just to check the inverse is correct or not .
Identity_matrix = np.dot(A,inverse)
Identity_matrix

# transpose
transpose_matrix = A.T
transpose_matrix

# eigen values 
#A=[[1,2],[1,3]] for understanding so when you print the B then first you get the eigen values then eigen vectors
#B =np.linalg.eig(A)
#B
eigen = np.linalg.eig(A)
eigen

# Jacobian example 

import sympy as sym

def Jacobian(v_str, expression):
    vars = sym.symbols(v_str)
    f = sym.sympify(expression)
    J = sym.zeros(len(f),len(vars))
    H = sym.zeros(len(f),len(vars))
    for i, fi in enumerate(f):
        for j, s in enumerate(vars):
            J[i,j] = sym.diff(fi, s)
            H[i,j] = sym.diff(J[i,j])
    print("Hessian matrix",H)
    return J

Answer = Jacobian('x1 x2', ['2*x1^2 + 3*x2','2*x1 - 3*x2'])
print("Jacobian", Answer)

# hessian matrix

from sklearn import preprocessing 
df = pd.read_csv("diabetes_data.csv")
df.head(5)

x=df.iloc[:,:-1]

y=df.iloc[:,16]

from sklearn.preprocessing import LabelEncoder
labelencoder=LabelEncoder()
x['Gender']=labelencoder.fit_transform(x['Gender'])
x['Polyuria']=labelencoder.fit_transform(x['Polyuria'])
x['Polydipsia']=labelencoder.fit_transform(x['Polydipsia'])
x['sudden weight loss']=labelencoder.fit_transform(x['sudden weight loss'])
x['weakness']=labelencoder.fit_transform(x['weakness'])
x['Polyphagia']=labelencoder.fit_transform(x['Polyphagia'])
x['Genital thrush']=labelencoder.fit_transform(x['Genital thrush'])
x['visual blurring']=labelencoder.fit_transform(x['visual blurring'])
x['Itching']=labelencoder.fit_transform(x['Itching'])
x['Irritability']=labelencoder.fit_transform(x['Irritability'])
x['delayed healing']=labelencoder.fit_transform(x['delayed healing'])
x['partial paresis']=labelencoder.fit_transform(x['partial paresis'])
x['muscle stiffness']=labelencoder.fit_transform(x['muscle stiffness'])
x['Alopecia']=labelencoder.fit_transform(x['Alopecia'])
x['Obesity']=labelencoder.fit_transform(x['Obesity'])
x.head(5)

from sklearn.preprocessing import LabelEncoder
labelencoder=LabelEncoder()
y=labelencoder.fit_transform(y)
y

# Scaling the data of Age column
from sklearn.preprocessing import StandardScaler
mean_age = df.Age.dropna().mean()
max_age = df.Age.dropna().max()
min_age = df.Age.dropna().min()
x['Age'] = x['Age'].apply(lambda z: (z - mean_age ) / (max_age -min_age ))

df.head(10)

# spliting the datset into traing and testing datset
from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.4, random_state=42)
x_train

# training the model

from sklearn.linear_model import LogisticRegression
model= LogisticRegression()
model.fit(x_train,y_train)

model.predict(x_test)

model.score(x_test,y_test)

model.predict_proba(x_test)

def sigmoid(z):
    sig_z= 1/(1+np.exp(-z))
    assert (z.shape==sig_z.shape), 'Error in sigmoid implementation. Check carefully'
    return sig_z
    
    
def predict(X,weights):
    '''Predict class for X.
    For the given dataset, predicted vector has only values 0/1
    Args:
        X : Numpy array (num_samples, num_features)
        weights : Model weights for logistic regression
    Returns:
        Binary predictions : (num_samples,)
    '''
    if( weights is None):
            raise Exception("Fit the model before prediction")

    ### START CODE HERE ###
    z=np.dot(x,weights)+bias
    logits = sigmoid(z)
    y_pred = logits
    ### END CODE HERE ###
    
    return y_pred

def cross_entropy():
            
    a = predict(x,weights)
    
    #y_pred = np.maximum(np.full(y_pred.shape, 1e-7), np.minimum(np.full(y_pred.shape, 1-1e-7), y_pred)) or can we fix the
    # zeros values with the above function 
        
    # compute cost of label 0 points
    ind = np.argwhere(y == 0)[:,1]
    cost = -np.sum(np.log(1 - a[:,ind]))
    
    # add cost on label 1 points
    ind = np.argwhere(y==1)[:,1]
    cost -= np.sum(np.log(a[:,ind]))
    
    return cost/y.size

def fit(self, X, y):
    '''Trains logistic regression model using gradient ascent
    to gain maximum likelihood on the training data
    Args:
        X : Numpy array (num_examples, num_features)
        y : Numpy array (num_examples, )
    Returns: VOID
    '''

    num_examples = X.shape[0]
    num_features = X.shape[1]

    ### START CODE HERE

    # Initialize weights with appropriate shape
    weights = np.zeros(num_features,)
    
    loss=[]

    # Perform gradient ascent
    for i in range(max_iterations):
        # Define the linear hypothesis(z) first
        # HINT: what is our hypothesis function in linear regression, remember?
        z = np.dot(weights)

        # Output probability value by appplying sigmoid on z
        y_pred = igmoid(z)
        # y_pred=self.predict(y_pred, threshold=0.5)

        # Calculate the gradient values
        # This is just vectorized efficient way of implementing gradient. Don't worry, we will discuss it later.
        gradient = np.mean((y-y_pred)*X.T, axis=1)

        # Update the weights
        # Caution: It is gradient ASCENT not descent
        weights = self.weights+self.learning_rate*gradient

        # Calculating log likelihood
        likelihood = self.log_likelihood(y, y_pred)

        self.likelihoods.append(likelihood)

    ### END CODE HERE

